{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e005e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.23 (main, Jun  5 2025, 13:40:20) \n",
      "[GCC 11.2.0]\n",
      "PyTorch version: 1.11.0+cu115\n",
      "OpenPCDet version: 0.6.0+8caccce+py4c5215f\n",
      "OpenPCDet path: /home/manas/OpenPCDet/pcdet/__init__.py\n",
      "YOLOv5 path: /home/manas/miniconda3/envs/sensor_fusion/lib/python3.9/site-packages/yolov5/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# --- Environment and Version Check ---\n",
    "import sys\n",
    "import torch\n",
    "import importlib\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "try:\n",
    "    import pcdet\n",
    "    print(f\"OpenPCDet version: {getattr(pcdet, '__version__', 'unknown')}\")\n",
    "    print(f\"OpenPCDet path: {pcdet.__file__}\")\n",
    "except ImportError:\n",
    "    print(\"OpenPCDet not installed!\")\n",
    "try:\n",
    "    yolov5_spec = importlib.util.find_spec('yolov5')#need to know more on this\n",
    "    if yolov5_spec is not None:\n",
    "        import yolov5\n",
    "        print(f\"YOLOv5 path: {yolov5.__file__}\")\n",
    "    else:\n",
    "        print(\"YOLOv5 not installed!\")\n",
    "except Exception as e:\n",
    "    print(f\"YOLOv5 import error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5277874",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install yolov5 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7964ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import Image\n",
    "import glob\n",
    "import open3d as o3d\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475dd11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 320\n",
      "Number of point clouds: 320\n",
      "Number of calibration files: 320\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Set working directory dynamically\n",
    "work_dir = os.getcwd()\n",
    "\n",
    "# Collect image, point cloud, and calibration file paths\n",
    "image_files = sorted(glob.glob(os.path.join(work_dir, \"data/img/*.png\")))\n",
    "point_files = sorted(glob.glob(os.path.join(work_dir, \"data/velodyne/*.bin\")))\n",
    "calib_files = sorted(glob.glob(os.path.join(work_dir, \"data/calib/*.txt\")))\n",
    "\n",
    "# (Optional) Skip label_files if not available\n",
    "#label_files = []  # We'll skip using this for now\n",
    "\n",
    "# Print summary\n",
    "print(\"Number of images:\", len(image_files))\n",
    "print(\"Number of point clouds:\", len(point_files))\n",
    "print(\"Number of calibration files:\", len(calib_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8148e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPCDet imported successfully\n",
      "YOLOv5 imported successfully\n",
      "PyTorch version: 1.11.0+cu115\n",
      "CUDA available: True\n",
      "Working directory: /home/manas/Sensor_fusion/Sensor-Fusion\n",
      "=== Data Validation ===\n",
      "Number of images: 320\n",
      "Number of point clouds: 320\n",
      "Number of calibration files: 320\n",
      "✓ All data counts match\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# OpenPCDet imports\n",
    "try:\n",
    "    from pcdet.config import cfg, cfg_from_yaml_file\n",
    "    from pcdet.datasets import DatasetTemplate\n",
    "    from pcdet.models import build_network, load_data_to_gpu\n",
    "    from pcdet.utils import common_utils\n",
    "    print(\"OpenPCDet imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"OpenPCDet import error: {e}\")\n",
    "    print(\"Please ensure OpenPCDet is properly installed\")\n",
    "\n",
    "# YOLOv5 imports\n",
    "try:\n",
    "    import yolov5\n",
    "    print(\"YOLOv5 imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"YOLOv5 import error: {e}\")\n",
    "    print(\"Please install YOLOv5: pip install yolov5\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 2: Setup Paths and Validate Data\n",
    "# =============================================================================\n",
    "\n",
    "# Set working directory\n",
    "work_dir = os.getcwd()\n",
    "print(f\"Working directory: {work_dir}\")\n",
    "\n",
    "# Data paths\n",
    "data_dir = os.path.join(work_dir, \"data\")\n",
    "image_dir = os.path.join(data_dir, \"img\")\n",
    "velodyne_dir = os.path.join(data_dir, \"velodyne\")\n",
    "calib_dir = os.path.join(data_dir, \"calib\")\n",
    "\n",
    "# Collect file paths\n",
    "image_files = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
    "point_files = sorted(glob.glob(os.path.join(velodyne_dir, \"*.bin\")))\n",
    "calib_files = sorted(glob.glob(os.path.join(calib_dir, \"*.txt\")))\n",
    "\n",
    "# Validate data consistency\n",
    "print(\"=== Data Validation ===\")\n",
    "print(f\"Number of images: {len(image_files)}\")\n",
    "print(f\"Number of point clouds: {len(point_files)}\")\n",
    "print(f\"Number of calibration files: {len(calib_files)}\")\n",
    "\n",
    "# Check if all counts match\n",
    "if len(image_files) == len(point_files) == len(calib_files):\n",
    "    print(\"✓ All data counts match\")\n",
    "    num_samples = len(image_files)\n",
    "else:\n",
    "    print(\"✗ Data count mismatch!\")\n",
    "    num_samples = min(len(image_files), len(point_files), len(calib_files))\n",
    "    print(f\"Using minimum count: {num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea66cf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71f9dc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setting up YOLOv5 Camera Detection ===\n",
      "✓ YOLOv5 model loaded successfully\n",
      "✓ YOLOv5 model moved to device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Setting up YOLOv5 Camera Detection ===\")\n",
    "\n",
    "# Load YOLOv5 model (using pretrained weights)\n",
    "try:\n",
    "    camera_model = yolov5.load('yolov5s.pt')  # You can use yolov5m.pt, yolov5l.pt, yolov5x.pt for better accuracy\n",
    "    camera_model.eval()\n",
    "    print(\"✓ YOLOv5 model loaded successfully\")\n",
    "    \n",
    "    # Set confidence threshold\n",
    "    camera_model.conf = 0.4  # confidence threshold\n",
    "    camera_model.iou = 0.45  # IoU threshold for NMS\n",
    "    \n",
    "    # Check device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    camera_model.to(device)\n",
    "    print(f\"✓ YOLOv5 model moved to device: {device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading YOLOv5: {e}\")\n",
    "    camera_model = None\n",
    "\n",
    "# Define COCO class names (YOLOv5 is trained on COCO dataset)\n",
    "COCO_CLASSES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck'\n",
    "]  # need to look more on this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b170453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 15:46:43,958   INFO  ==> Loading parameters from checkpoint /home/manas/Downloads/pointpillar_7728.pth to GPU\n",
      "2025-07-29 15:46:43,958   INFO  ==> Loading parameters from checkpoint /home/manas/Downloads/pointpillar_7728.pth to GPU\n",
      "2025-07-29 15:46:44,020   INFO  ==> Done (loaded 127/127)\n",
      "2025-07-29 15:46:44,020   INFO  ==> Done (loaded 127/127)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setting up PointPillars Model ===\n",
      "✓ PointPillars model initialized with your config and weights\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Setting up PointPillars Model ===\")\n",
    "\n",
    "# Manually specify your config and checkpoint paths\n",
    "config_path = \"/home/manas/OpenPCDet/tools/cfgs/kitti_models/pointpillar.yaml\"  # Update if needed\n",
    "ckpt_path = \"/home/manas/Downloads/pointpillar_7728.pth\"  # Update if needed\n",
    "\n",
    "try:\n",
    "    # Load config\n",
    "    from pcdet.config import cfg, cfg_from_yaml_file\n",
    "    cfg_from_yaml_file(config_path, cfg)\n",
    "    \n",
    "    # Build model\n",
    "    from pcdet.models import build_network\n",
    "    from pcdet.utils import common_utils\n",
    "    from pcdet.datasets import DatasetTemplate\n",
    "    dataset = DatasetTemplate(dataset_cfg=cfg.DATA_CONFIG, class_names=cfg.CLASS_NAMES, training=False, root_path=None, logger=common_utils.create_logger())\n",
    "    lidar_model = build_network(model_cfg=cfg.MODEL, num_class=len(cfg.CLASS_NAMES), dataset=dataset)\n",
    "    \n",
    "    # Load pretrained weights\n",
    "    lidar_model.load_params_from_file(ckpt_path, logger=common_utils.create_logger())\n",
    "    if torch.cuda.is_available():\n",
    "        lidar_model.cuda()\n",
    "    else:\n",
    "        lidar_model.cpu()\n",
    "    lidar_model.eval()\n",
    "    print(\"✓ PointPillars model initialized with your config and weights\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error initializing PointPillars: {e}\")\n",
    "    lidar_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "329282e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITTICalibration:\n",
    "    \"\"\"KITTI calibration file parser\"\"\"\n",
    "    \n",
    "    def __init__(self, calib_file):\n",
    "        \"\"\"Load calibration data from file\"\"\"\n",
    "        calibs = self.read_calib_file(calib_file)\n",
    "        \n",
    "        # Camera intrinsics and extrinsics\n",
    "        self.P2 = calibs['P2'].reshape(3, 4)  # Camera 2 projection matrix\n",
    "        self.R0_rect = calibs['R0_rect'].reshape(3, 3)  # Rectification matrix\n",
    "        self.Tr_velo_to_cam = calibs['Tr_velo_to_cam'].reshape(3, 4)  # Velodyne to camera\n",
    "        \n",
    "        # Create full transformation matrices\n",
    "        self.R0_rect_4x4 = np.eye(4)\n",
    "        self.R0_rect_4x4[:3, :3] = self.R0_rect\n",
    "        \n",
    "        self.Tr_velo_to_cam_4x4 = np.eye(4)\n",
    "        self.Tr_velo_to_cam_4x4[:3, :] = self.Tr_velo_to_cam\n",
    "        \n",
    "    def read_calib_file(self, filepath):\n",
    "        \"\"\"Read KITTI calibration file\"\"\"\n",
    "        data = {}\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                if len(line) == 0: continue\n",
    "                key, value = line.split(':', 1)\n",
    "                try:\n",
    "                    data[key] = np.array([float(x) for x in value.split()])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return data\n",
    "    \n",
    "    def project_velo_to_image(self, pts_3d_velo):\n",
    "        \"\"\"Project velodyne points to camera image\"\"\"\n",
    "        # Convert to homogeneous coordinates\n",
    "        pts_3d_velo_homo = np.hstack([pts_3d_velo, np.ones((pts_3d_velo.shape[0], 1))])\n",
    "        \n",
    "        # Transform: Velodyne -> Camera -> Rectified Camera -> Image\n",
    "        pts_3d_cam = pts_3d_velo_homo @ self.Tr_velo_to_cam_4x4.T\n",
    "        pts_3d_rect = pts_3d_cam @ self.R0_rect_4x4.T\n",
    "        pts_2d_image = pts_3d_rect @ self.P2.T\n",
    "        \n",
    "        # Normalize homogeneous coordinates\n",
    "        pts_2d_image[:, 0] /= pts_2d_image[:, 2]\n",
    "        pts_2d_image[:, 1] /= pts_2d_image[:, 2]\n",
    "        \n",
    "        return pts_2d_image[:, :2], pts_3d_rect[:, 2]  # Return 2D points and depths\n",
    "\n",
    "def load_kitti_pointcloud(bin_file):\n",
    "    \"\"\"Load KITTI point cloud from binary file\"\"\"\n",
    "    points = np.fromfile(bin_file, dtype=np.float32).reshape(-1, 4)\n",
    "    return points[:, :3]  # Return only XYZ coordinates\n",
    "\n",
    "def process_camera_detections(image_path, model):\n",
    "    \"\"\"Process camera image and return detections\"\"\"\n",
    "    if model is None:\n",
    "        return [], None\n",
    "    \n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run inference\n",
    "        results = model(image_rgb)\n",
    "        \n",
    "        # Extract detections\n",
    "        detections = []\n",
    "        if len(results.xyxy[0]) > 0:\n",
    "            for detection in results.xyxy[0].cpu().numpy():\n",
    "                x1, y1, x2, y2, conf, cls = detection\n",
    "                class_name = COCO_CLASSES[int(cls)]\n",
    "                \n",
    "                detections.append({\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'confidence': conf,\n",
    "                    'class': class_name,\n",
    "                    'class_id': int(cls)\n",
    "                })\n",
    "        \n",
    "        return detections, image_rgb\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in camera detection: {e}\")\n",
    "        return [], None\n",
    "\n",
    "def prepare_lidar_input(points, dataset):\n",
    "    \"\"\"\n",
    "    Prepare a single point cloud for PointPillars inference using OpenPCDet's dataset utilities.\n",
    "    \"\"\"\n",
    "    # Create input dict as expected by OpenPCDet\n",
    "    input_dict = {\n",
    "        'points': points,\n",
    "        'frame_id': 0,\n",
    "    }\n",
    "    # Use the dataset's prepare_data method to get the correct batch dict\n",
    "    data_dict = dataset.prepare_data(input_dict)\n",
    "    # Collate into batch (OpenPCDet expects batched input)\n",
    "    batch_dict = dataset.collate_batch([data_dict])\n",
    "    return batch_dict\n",
    "\n",
    "# Update process_lidar_detections to use this utility\n",
    "def process_lidar_detections(points, model, dataset, device='cuda'):\n",
    "    \"\"\"Process LiDAR point cloud and return detections using OpenPCDet pipeline.\"\"\"\n",
    "    if model is None or dataset is None:\n",
    "        return []\n",
    "    try:\n",
    "        batch_dict = prepare_lidar_input(points, dataset)\n",
    "        # Move all tensors to device\n",
    "        for k in batch_dict:\n",
    "            if isinstance(batch_dict[k], torch.Tensor):\n",
    "                batch_dict[k] = batch_dict[k].to(device)\n",
    "        with torch.no_grad():\n",
    "            pred_dicts, _ = model(batch_dict)\n",
    "        detections = []\n",
    "        if len(pred_dicts) > 0:\n",
    "            pred_dict = pred_dicts[0]\n",
    "            if 'pred_boxes' in pred_dict:\n",
    "                boxes = pred_dict['pred_boxes'].cpu().numpy()\n",
    "                scores = pred_dict['pred_scores'].cpu().numpy()\n",
    "                labels = pred_dict['pred_labels'].cpu().numpy()\n",
    "                for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n",
    "                    if score > 0.3:\n",
    "                        detections.append({\n",
    "                            'bbox_3d': box,\n",
    "                            'confidence': score,\n",
    "                            'class': cfg.CLASS_NAMES[label-1],\n",
    "                            'class_id': label-1\n",
    "                        })\n",
    "        return detections\n",
    "    except Exception as e:\n",
    "        print(f\"Error in LiDAR detection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e6ae815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Late fusion processor initialized\n"
     ]
    }
   ],
   "source": [
    "class LateFusionProcessor:\n",
    "    \"\"\"Late fusion processor for camera and LiDAR detections\"\"\"\n",
    "    \n",
    "    def __init__(self, camera_model, lidar_model):\n",
    "        self.camera_model = camera_model\n",
    "        self.lidar_model = lidar_model\n",
    "        \n",
    "        # Fusion parameters\n",
    "        self.iou_threshold = 0.1\n",
    "        self.confidence_weights = {'camera': 0.6, 'lidar': 0.6}\n",
    "        \n",
    "    def compute_2d_iou(self, box1, box2):\n",
    "        \"\"\"Compute IoU between two 2D bounding boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = (x2 - x1) * (y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def project_3d_to_2d_bbox(self, bbox_3d, calib):\n",
    "        \"\"\"Project 3D bounding box to 2D image coordinates\"\"\"\n",
    "        # Extract 3D box parameters (x, y, z, w, l, h, rotation)\n",
    "        x, y, z, w, l, h, rot = bbox_3d[:7]\n",
    "        \n",
    "        # Create 3D box corners\n",
    "        corners_3d = self.get_3d_box_corners(x, y, z, w, l, h, rot)\n",
    "        \n",
    "        # Project to image\n",
    "        corners_2d, depths = calib.project_velo_to_image(corners_3d)\n",
    "        \n",
    "        # Get 2D bounding box\n",
    "        if len(corners_2d) > 0:\n",
    "            x_min, y_min = np.min(corners_2d, axis=0)\n",
    "            x_max, y_max = np.max(corners_2d, axis=0)\n",
    "            return [x_min, y_min, x_max, y_max]\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_3d_box_corners(self, x, y, z, w, l, h, rot):\n",
    "        \"\"\"Get 3D box corners from center point and dimensions\"\"\"\n",
    "        # Create box corners in object coordinate system\n",
    "        corners = np.array([\n",
    "            [-l/2, -w/2, -h/2], [l/2, -w/2, -h/2],\n",
    "            [l/2, w/2, -h/2], [-l/2, w/2, -h/2],\n",
    "            [-l/2, -w/2, h/2], [l/2, -w/2, h/2],\n",
    "            [l/2, w/2, h/2], [-l/2, w/2, h/2]\n",
    "        ])\n",
    "        \n",
    "        # Rotation matrix around Z-axis\n",
    "        cos_rot = np.cos(rot)\n",
    "        sin_rot = np.sin(rot)\n",
    "        rot_matrix = np.array([\n",
    "            [cos_rot, -sin_rot, 0],\n",
    "            [sin_rot, cos_rot, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        # Rotate and translate\n",
    "        corners = corners @ rot_matrix.T\n",
    "        corners[:, 0] += x\n",
    "        corners[:, 1] += y\n",
    "        corners[:, 2] += z\n",
    "        \n",
    "        return corners\n",
    "    \n",
    "    def fuse_detections(self, camera_detections, lidar_detections, calib):\n",
    "        \"\"\"Perform late fusion of camera and LiDAR detections\"\"\"\n",
    "        fused_detections = []\n",
    "        used_lidar = set()\n",
    "        \n",
    "        # Process camera detections\n",
    "        for cam_det in camera_detections:\n",
    "            best_match = None\n",
    "            best_iou = 0\n",
    "            best_lidar_idx = -1\n",
    "            \n",
    "            # Find matching LiDAR detection\n",
    "            for i, lidar_det in enumerate(lidar_detections):\n",
    "                if i in used_lidar:\n",
    "                    continue\n",
    "                \n",
    "                # Project 3D box to 2D\n",
    "                bbox_2d = self.project_3d_to_2d_bbox(lidar_det['bbox_3d'], calib)\n",
    "                \n",
    "                if bbox_2d is not None:\n",
    "                    iou = self.compute_2d_iou(cam_det['bbox'], bbox_2d)\n",
    "                    \n",
    "                    if iou > best_iou and iou > self.iou_threshold:\n",
    "                        best_iou = iou\n",
    "                        best_match = lidar_det\n",
    "                        best_lidar_idx = i\n",
    "            \n",
    "            # Create fused detection\n",
    "            if best_match is not None:\n",
    "                # Fuse confidences\n",
    "                fused_confidence = (\n",
    "                    self.confidence_weights['camera'] * cam_det['confidence'] +\n",
    "                    self.confidence_weights['lidar'] * best_match['confidence']\n",
    "                )\n",
    "                \n",
    "                fused_detection = {\n",
    "                    'bbox_2d': cam_det['bbox'],\n",
    "                    'bbox_3d': best_match['bbox_3d'],\n",
    "                    'confidence': fused_confidence,\n",
    "                    'camera_confidence': cam_det['confidence'],\n",
    "                    'lidar_confidence': best_match['confidence'],\n",
    "                    'class': cam_det['class'],\n",
    "                    'fusion_type': 'camera_lidar',\n",
    "                    'iou': best_iou\n",
    "                }\n",
    "                \n",
    "                fused_detections.append(fused_detection)\n",
    "                used_lidar.add(best_lidar_idx)\n",
    "            else:\n",
    "                # Camera-only detection\n",
    "                fused_detection = {\n",
    "                    'bbox_2d': cam_det['bbox'],\n",
    "                    'bbox_3d': None,\n",
    "                    'confidence': cam_det['confidence'],\n",
    "                    'camera_confidence': cam_det['confidence'],\n",
    "                    'lidar_confidence': 0.0,\n",
    "                    'class': cam_det['class'],\n",
    "                    'fusion_type': 'camera_only',\n",
    "                    'iou': 0.0\n",
    "                }\n",
    "                fused_detections.append(fused_detection)\n",
    "        \n",
    "        # Add unmatched LiDAR detections\n",
    "        for i, lidar_det in enumerate(lidar_detections):\n",
    "            if i not in used_lidar:\n",
    "                fused_detection = {\n",
    "                    'bbox_2d': None,\n",
    "                    'bbox_3d': lidar_det['bbox_3d'],\n",
    "                    'confidence': lidar_det['confidence'],\n",
    "                    'camera_confidence': 0.0,\n",
    "                    'lidar_confidence': lidar_det['confidence'],\n",
    "                    'class': lidar_det['class'],\n",
    "                    'fusion_type': 'lidar_only',\n",
    "                    'iou': 0.0\n",
    "                }\n",
    "                fused_detections.append(fused_detection)\n",
    "        \n",
    "        return fused_detections\n",
    "\n",
    "# Initialize fusion processor\n",
    "fusion_processor = LateFusionProcessor(camera_model, lidar_model)\n",
    "print(\"✓ Late fusion processor initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f788d421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Visualization functions defined\n"
     ]
    }
   ],
   "source": [
    "def visualize_detections(image, camera_dets, lidar_dets, fused_dets, calib, save_path=None):\n",
    "    \"\"\"Visualize camera, LiDAR, and fused detections\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, 0].imshow(image)\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Camera detections\n",
    "    img_cam = image.copy()\n",
    "    for det in camera_dets:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(img_cam, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)\n",
    "        cv2.putText(img_cam, f\"{det['class']}: {det['confidence']:.2f}\", \n",
    "                   (int(bbox[0]), int(bbox[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    axes[0, 1].imshow(img_cam)\n",
    "    axes[0, 1].set_title(f'Camera Detections ({len(camera_dets)})')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # LiDAR detections projected to image\n",
    "    img_lidar = image.copy()\n",
    "    for det in lidar_dets:\n",
    "        bbox_2d = fusion_processor.project_3d_to_2d_bbox(det['bbox_3d'], calib)\n",
    "        if bbox_2d is not None:\n",
    "            cv2.rectangle(img_lidar, (int(bbox_2d[0]), int(bbox_2d[1])), \n",
    "                         (int(bbox_2d[2]), int(bbox_2d[3])), (0, 255, 0), 2)\n",
    "            cv2.putText(img_lidar, f\"{det['class']}: {det['confidence']:.2f}\", \n",
    "                       (int(bbox_2d[0]), int(bbox_2d[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    axes[1, 0].imshow(img_lidar)\n",
    "    axes[1, 0].set_title(f'LiDAR Detections Projected ({len(lidar_dets)})')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Fused detections\n",
    "    img_fused = image.copy()\n",
    "    colors = {'camera_lidar': (255, 0, 255), 'camera_only': (255, 0, 0), 'lidar_only': (0, 255, 0)}\n",
    "    \n",
    "    for det in fused_dets:\n",
    "        color = colors[det['fusion_type']]\n",
    "        \n",
    "        if det['bbox_2d'] is not None:\n",
    "            bbox = det['bbox_2d']\n",
    "            cv2.rectangle(img_fused, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), color, 2)\n",
    "            cv2.putText(img_fused, f\"{det['class']}: {det['confidence']:.2f} ({det['fusion_type']})\", \n",
    "                       (int(bbox[0]), int(bbox[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        elif det['bbox_3d'] is not None:\n",
    "            # Project LiDAR-only detection\n",
    "            bbox_2d = fusion_processor.project_3d_to_2d_bbox(det['bbox_3d'], calib)\n",
    "            if bbox_2d is not None:\n",
    "                cv2.rectangle(img_fused, (int(bbox_2d[0]), int(bbox_2d[1])), \n",
    "                             (int(bbox_2d[2]), int(bbox_2d[3])), color, 2)\n",
    "                cv2.putText(img_fused, f\"{det['class']}: {det['confidence']:.2f} ({det['fusion_type']})\", \n",
    "                           (int(bbox_2d[0]), int(bbox_2d[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    axes[1, 1].imshow(img_fused)\n",
    "    axes[1, 1].set_title(f'Fused Detections ({len(fused_dets)})')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='red', alpha=0.7, label='Camera Only'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='green', alpha=0.7, label='LiDAR Only'),\n",
    "        plt.Rectangle((0, 0), 1, 1, facecolor='magenta', alpha=0.7, label='Camera + LiDAR')\n",
    "    ]\n",
    "    axes[1, 1].legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"✓ Visualization saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_detection_statistics(results):\n",
    "    \"\"\"Plot statistics of detection results\"\"\"\n",
    "    fusion_types = ['camera_only', 'lidar_only', 'camera_lidar']\n",
    "    type_counts = {ft: 0 for ft in fusion_types}\n",
    "    confidences = {ft: [] for ft in fusion_types}\n",
    "    \n",
    "    for result in results:\n",
    "        for det in result['fused_detections']:\n",
    "            fusion_type = det['fusion_type']\n",
    "            type_counts[fusion_type] += 1\n",
    "            confidences[fusion_type].append(det['confidence'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Detection type distribution\n",
    "    axes[0].bar(type_counts.keys(), type_counts.values(), \n",
    "                color=['red', 'green', 'magenta'], alpha=0.7)\n",
    "    axes[0].set_title('Detection Type Distribution')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Confidence distribution\n",
    "    for ft, confs in confidences.items():\n",
    "        if confs:\n",
    "            axes[1].hist(confs, alpha=0.7, label=ft, bins=20)\n",
    "    \n",
    "    axes[1].set_title('Confidence Distribution by Fusion Type')\n",
    "    axes[1].set_xlabel('Confidence')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4171d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PointPillars debug check...\n",
      "=== PointPillars Debug Info ===\n",
      "✓ LiDAR model loaded\n",
      "  Model type: <class 'pcdet.models.detectors.pointpillar.PointPillar'>\n",
      "  Model device: cuda:0\n",
      "✓ Dataset available\n",
      "  Dataset type: <class 'pcdet.datasets.dataset.DatasetTemplate'>\n",
      "  Class names: ['Car', 'Pedestrian', 'Cyclist']\n",
      "✓ Config loaded\n",
      "  Number of classes: 3\n",
      "  Classes: ['Car', 'Pedestrian', 'Cyclist']\n",
      "\n",
      "✓ PointPillars setup looks good. You can now run:\n",
      "test_result = process_single_sample_fixed(3)\n"
     ]
    }
   ],
   "source": [
    "def prepare_lidar_input(points, dataset):\n",
    "    \"\"\"\n",
    "    Prepare a single point cloud for PointPillars inference using OpenPCDet's dataset utilities.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure points are in the correct format (N, 4) with intensity\n",
    "        if points.shape[1] == 3:\n",
    "            # Add dummy intensity column if missing\n",
    "            intensity = np.ones((points.shape[0], 1), dtype=points.dtype)\n",
    "            points = np.hstack([points, intensity])\n",
    "        \n",
    "        # Create input dict as expected by OpenPCDet\n",
    "        input_dict = {\n",
    "            'points': points,\n",
    "            'frame_id': 0,\n",
    "        }\n",
    "        \n",
    "        # Use the dataset's prepare_data method to get the correct batch dict\n",
    "        data_dict = dataset.prepare_data(input_dict)\n",
    "        \n",
    "        # Collate into batch (OpenPCDet expects batched input)\n",
    "        batch_dict = dataset.collate_batch([data_dict])\n",
    "        \n",
    "        return batch_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_lidar_input: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_lidar_detections(points, model, dataset, device='cuda'):\n",
    "    \"\"\"Process LiDAR point cloud and return detections using OpenPCDet pipeline.\"\"\"\n",
    "    if model is None or dataset is None:\n",
    "        print(\"Model or dataset is None\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Prepare input data\n",
    "        batch_dict = prepare_lidar_input(points, dataset)\n",
    "        if batch_dict is None:\n",
    "            print(\"Failed to prepare LiDAR input\")\n",
    "            return []\n",
    "        \n",
    "        # Move all tensors to device\n",
    "        for k in batch_dict:\n",
    "            if isinstance(batch_dict[k], torch.Tensor):\n",
    "                batch_dict[k] = batch_dict[k].to(device)\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            pred_dicts, _ = model(batch_dict)\n",
    "        \n",
    "        detections = []\n",
    "        if len(pred_dicts) > 0:\n",
    "            pred_dict = pred_dicts[0]\n",
    "            \n",
    "            if 'pred_boxes' in pred_dict:\n",
    "                boxes = pred_dict['pred_boxes'].cpu().numpy()\n",
    "                scores = pred_dict['pred_scores'].cpu().numpy()\n",
    "                labels = pred_dict['pred_labels'].cpu().numpy()\n",
    "                \n",
    "                # Filter detections by confidence threshold\n",
    "                confidence_threshold = 0.3\n",
    "                \n",
    "                for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n",
    "                    if score > confidence_threshold:\n",
    "                        # Ensure label index is valid\n",
    "                        if label > 0 and label <= len(cfg.CLASS_NAMES):\n",
    "                            class_name = cfg.CLASS_NAMES[label-1]\n",
    "                        else:\n",
    "                            class_name = f\"class_{label}\"\n",
    "                        \n",
    "                        detections.append({\n",
    "                            'bbox_3d': box,\n",
    "                            'confidence': float(score),\n",
    "                            'class': class_name,\n",
    "                            'class_id': int(label-1)\n",
    "                        })\n",
    "        \n",
    "        print(f\"PointPillars detected {len(detections)} objects\")\n",
    "        return detections\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LiDAR detection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def process_single_sample_fixed(sample_idx=3):\n",
    "    \"\"\"Fixed version of process_single_sample with proper dataset passing\"\"\"\n",
    "    \n",
    "    if sample_idx >= num_samples:\n",
    "        print(f\"Error: Sample index {sample_idx} out of range (0-{num_samples-1})\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"=== Processing Sample {sample_idx} ===\")\n",
    "    \n",
    "    # Load data paths\n",
    "    image_path = image_files[sample_idx]\n",
    "    point_path = point_files[sample_idx]\n",
    "    calib_path = calib_files[sample_idx]\n",
    "    \n",
    "    print(f\"Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"Point cloud: {os.path.basename(point_path)}\")\n",
    "    print(f\"Calibration: {os.path.basename(calib_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Load calibration\n",
    "        calib = KITTICalibration(calib_path)\n",
    "        print(\"✓ Calibration loaded\")\n",
    "        \n",
    "        # Load point cloud\n",
    "        points = load_kitti_pointcloud(point_path)\n",
    "        print(f\"✓ Point cloud loaded: {points.shape[0]} points\")\n",
    "        \n",
    "        # Process camera detections\n",
    "        print(\"Processing camera detections...\")\n",
    "        camera_detections, image = process_camera_detections(image_path, camera_model)\n",
    "        print(f\"✓ Camera detections: {len(camera_detections)}\")\n",
    "        \n",
    "        # Process LiDAR detections (fixed with dataset parameter)\n",
    "        print(\"Processing LiDAR detections...\")\n",
    "        lidar_detections = process_lidar_detections(points, lidar_model, dataset)  # Added dataset parameter\n",
    "        print(f\"✓ LiDAR detections: {len(lidar_detections)}\")\n",
    "        \n",
    "        # Perform fusion\n",
    "        print(\"Performing late fusion...\")\n",
    "        fused_detections = fusion_processor.fuse_detections(\n",
    "            camera_detections, lidar_detections, calib\n",
    "        )\n",
    "        print(f\"✓ Fused detections: {len(fused_detections)}\")\n",
    "        \n",
    "        # Print detection details\n",
    "        print(\"\\n=== Detection Details ===\")\n",
    "        print(\"Camera detections:\")\n",
    "        for i, det in enumerate(camera_detections):\n",
    "            print(f\"  {i+1}. {det['class']} (conf: {det['confidence']:.3f})\")\n",
    "        \n",
    "        print(\"LiDAR detections:\")\n",
    "        for i, det in enumerate(lidar_detections):\n",
    "            print(f\"  {i+1}. {det['class']} (conf: {det['confidence']:.3f})\")\n",
    "            \n",
    "        print(\"Fused detections:\")\n",
    "        for i, det in enumerate(fused_detections):\n",
    "            print(f\"  {i+1}. {det['class']} (conf: {det['confidence']:.3f}, type: {det['fusion_type']})\")\n",
    "        \n",
    "        # Visualize results\n",
    "        if image is not None:\n",
    "            visualize_detections(\n",
    "                image, camera_detections, lidar_detections, fused_detections, calib,\n",
    "                save_path=f\"fusion_result_sample_{sample_idx}.png\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'sample_idx': sample_idx,\n",
    "            'camera_detections': camera_detections,\n",
    "            'lidar_detections': lidar_detections,\n",
    "            'fused_detections': fused_detections,\n",
    "            'image': image,\n",
    "            'points': points,\n",
    "            'calibration': calib\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing sample {sample_idx}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Additional debugging function to check model and dataset\n",
    "def debug_pointpillars_setup():\n",
    "    \"\"\"Debug function to check PointPillars model and dataset setup\"\"\"\n",
    "    print(\"=== PointPillars Debug Info ===\")\n",
    "    \n",
    "    # Check model\n",
    "    if lidar_model is None:\n",
    "        print(\"✗ LiDAR model is None\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✓ LiDAR model loaded\")\n",
    "        print(f\"  Model type: {type(lidar_model)}\")\n",
    "        print(f\"  Model device: {next(lidar_model.parameters()).device}\")\n",
    "    \n",
    "    # Check dataset\n",
    "    if 'dataset' not in globals():\n",
    "        print(\"✗ Dataset not found in globals\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"✓ Dataset available\")\n",
    "        print(f\"  Dataset type: {type(dataset)}\")\n",
    "        print(f\"  Class names: {cfg.CLASS_NAMES}\")\n",
    "    \n",
    "    # Check config\n",
    "    print(f\"✓ Config loaded\")\n",
    "    print(f\"  Number of classes: {len(cfg.CLASS_NAMES)}\")\n",
    "    print(f\"  Classes: {cfg.CLASS_NAMES}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Test the debug function\n",
    "print(\"Running PointPillars debug check...\")\n",
    "debug_result = debug_pointpillars_setup()\n",
    "\n",
    "if debug_result:\n",
    "    print(\"\\n✓ PointPillars setup looks good. You can now run:\")\n",
    "    print(\"test_result = process_single_sample_fixed(3)\")\n",
    "else:\n",
    "    print(\"\\n✗ Issues found with PointPillars setup. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a529583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Fixed LiDAR Processing ===\n",
      "=== Processing Sample 3 (v2) ===\n",
      "Image: 000003.png\n",
      "Point cloud: 000003.bin\n",
      "Calibration: 000003.txt\n",
      "✓ Calibration loaded\n",
      "✓ Point cloud loaded: 121795 points\n",
      "Processing camera detections...\n",
      "✓ Camera detections: 4\n",
      "Processing LiDAR detections (v2)...\n",
      "Batch dict keys: ['points', 'frame_id', 'lidar_aug_matrix', 'use_lead_xyz', 'voxels', 'voxel_coords', 'voxel_num_points', 'batch_size']\n",
      "Converted and moved points to cuda, shape: (63289, 5)\n",
      "Converted and moved frame_id to cuda, shape: (1,)\n",
      "Converted and moved lidar_aug_matrix to cuda, shape: (1, 4, 4)\n",
      "Converted and moved use_lead_xyz to cuda, shape: (1,)\n",
      "Converted and moved voxels to cuda, shape: (8494, 32, 4)\n",
      "Converted and moved voxel_coords to cuda, shape: (8494, 4)\n",
      "Converted and moved voxel_num_points to cuda, shape: (8494,)\n",
      "voxel_features.shape: torch.Size([8494, 32, 4])\n",
      "voxel_num_points.shape: torch.Size([8494])\n",
      "Raw predictions: 20 boxes, score range: [0.104, 0.394]\n",
      "PointPillars detected 1 objects (after filtering)\n",
      "✓ LiDAR detections: 1\n",
      "Performing late fusion...\n",
      "✓ Fused detections: 5\n",
      "\n",
      "=== Detection Details ===\n",
      "Camera detections:\n",
      "  1. car (conf: 0.891)\n",
      "  2. car (conf: 0.609)\n",
      "  3. truck (conf: 0.529)\n",
      "  4. car (conf: 0.476)\n",
      "LiDAR detections:\n",
      "  1. Car (conf: 0.394)\n",
      "Fused detections:\n",
      "  1. car (conf: 0.891, type: camera_only)\n",
      "  2. car (conf: 0.609, type: camera_only)\n",
      "  3. truck (conf: 0.529, type: camera_only)\n",
      "  4. car (conf: 0.476, type: camera_only)\n",
      "  5. Car (conf: 0.394, type: lidar_only)\n",
      "✓ Visualization saved to: fusion_result_sample_3_v2.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def prepare_lidar_input_fixed(points, dataset):\n",
    "    \"\"\"\n",
    "    Fixed version: Prepare a single point cloud for PointPillars inference with proper tensor handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure points are in the correct format (N, 4) with intensity\n",
    "        if points.shape[1] == 3:\n",
    "            # Add dummy intensity column if missing\n",
    "            intensity = np.ones((points.shape[0], 1), dtype=points.dtype)\n",
    "            points = np.hstack([points, intensity])\n",
    "        \n",
    "        # Ensure points are float32\n",
    "        points = points.astype(np.float32)\n",
    "        \n",
    "        # Create input dict as expected by OpenPCDet\n",
    "        input_dict = {\n",
    "            'points': points,\n",
    "            'frame_id': 0,\n",
    "        }\n",
    "        \n",
    "        # Use the dataset's prepare_data method to get the correct batch dict\n",
    "        data_dict = dataset.prepare_data(input_dict)\n",
    "        \n",
    "        # Convert numpy arrays to tensors before collating\n",
    "        for key, value in data_dict.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                data_dict[key] = torch.from_numpy(value)\n",
    "        \n",
    "        # Collate into batch (OpenPCDet expects batched input)\n",
    "        batch_dict = dataset.collate_batch([data_dict])\n",
    "        \n",
    "        return batch_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_lidar_input_fixed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def process_lidar_detections_fixed(points, model, dataset, device='cuda'):\n",
    "    \"\"\"Fixed version: Process LiDAR point cloud with proper tensor handling.\"\"\"\n",
    "    if model is None or dataset is None:\n",
    "        print(\"Model or dataset is None\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Prepare input data\n",
    "        batch_dict = prepare_lidar_input_fixed(points, dataset)\n",
    "        if batch_dict is None:\n",
    "            print(\"Failed to prepare LiDAR input\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Batch dict keys: {list(batch_dict.keys())}\")\n",
    "        \n",
    "        # Move all tensors to device and ensure they are tensors\n",
    "        device_obj = torch.device(device)\n",
    "        for k, v in batch_dict.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch_dict[k] = v.to(device_obj)\n",
    "                print(f\"Moved {k} to {device_obj}, shape: {v.shape}, dtype: {v.dtype}\")\n",
    "            elif isinstance(v, np.ndarray):\n",
    "                # Convert remaining numpy arrays to tensors\n",
    "                batch_dict[k] = torch.from_numpy(v).to(device_obj)\n",
    "                print(f\"Converted and moved {k} to {device_obj}, shape: {v.shape}\")\n",
    "            elif isinstance(v, list):\n",
    "                # Handle lists (e.g., batch_size info)\n",
    "                print(f\"List field {k}: {v}\")\n",
    "        \n",
    "        # Run inference\n",
    "        model.eval()  # Ensure model is in eval mode\n",
    "        with torch.no_grad():\n",
    "            pred_dicts, _ = model(batch_dict)\n",
    "        \n",
    "        detections = []\n",
    "        if len(pred_dicts) > 0:\n",
    "            pred_dict = pred_dicts[0]\n",
    "            \n",
    "            if 'pred_boxes' in pred_dict and len(pred_dict['pred_boxes']) > 0:\n",
    "                boxes = pred_dict['pred_boxes'].cpu().numpy()\n",
    "                scores = pred_dict['pred_scores'].cpu().numpy()\n",
    "                labels = pred_dict['pred_labels'].cpu().numpy()\n",
    "                \n",
    "                print(f\"Raw predictions: {len(boxes)} boxes, score range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "                \n",
    "                # Filter detections by confidence threshold\n",
    "                confidence_threshold = 0.3\n",
    "                \n",
    "                for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):\n",
    "                    if score > confidence_threshold:\n",
    "                        # Ensure label index is valid\n",
    "                        if label > 0 and label <= len(cfg.CLASS_NAMES):\n",
    "                            class_name = cfg.CLASS_NAMES[label-1]\n",
    "                        else:\n",
    "                            class_name = f\"class_{label}\"\n",
    "                        \n",
    "                        detections.append({\n",
    "                            'bbox_3d': box,\n",
    "                            'confidence': float(score),\n",
    "                            'class': class_name,\n",
    "                            'class_id': int(label-1)\n",
    "                        })\n",
    "            else:\n",
    "                print(\"No 'pred_boxes' in prediction or empty predictions\")\n",
    "        \n",
    "        print(f\"PointPillars detected {len(detections)} objects (after filtering)\")\n",
    "        return detections\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in LiDAR detection: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "def process_single_sample_v2(sample_idx=3):\n",
    "    \"\"\"Version 2 of process_single_sample with fixed LiDAR processing\"\"\"\n",
    "    \n",
    "    if sample_idx >= num_samples:\n",
    "        print(f\"Error: Sample index {sample_idx} out of range (0-{num_samples-1})\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"=== Processing Sample {sample_idx} (v2) ===\")\n",
    "    \n",
    "    # Load data paths\n",
    "    image_path = image_files[sample_idx]\n",
    "    point_path = point_files[sample_idx]\n",
    "    calib_path = calib_files[sample_idx]\n",
    "    \n",
    "    print(f\"Image: {os.path.basename(image_path)}\")\n",
    "    print(f\"Point cloud: {os.path.basename(point_path)}\")\n",
    "    print(f\"Calibration: {os.path.basename(calib_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Load calibration\n",
    "        calib = KITTICalibration(calib_path)\n",
    "        print(\"✓ Calibration loaded\")\n",
    "        \n",
    "        # Load point cloud\n",
    "        points = load_kitti_pointcloud(point_path)\n",
    "        print(f\"✓ Point cloud loaded: {points.shape[0]} points\")\n",
    "        \n",
    "        # Process camera detections\n",
    "        print(\"Processing camera detections...\")\n",
    "        camera_detections, image = process_camera_detections(image_path, camera_model)\n",
    "        print(f\"✓ Camera detections: {len(camera_detections)}\")\n",
    "        \n",
    "        # Process LiDAR detections with fixed function\n",
    "        print(\"Processing LiDAR detections (v2)...\")\n",
    "        lidar_detections = process_lidar_detections_fixed(points, lidar_model, dataset)\n",
    "        print(f\"✓ LiDAR detections: {len(lidar_detections)}\")\n",
    "        \n",
    "        # Perform fusion\n",
    "        print(\"Performing late fusion...\")\n",
    "        fused_detections = fusion_processor.fuse_detections(\n",
    "            camera_detections, lidar_detections, calib\n",
    "        )\n",
    "        print(f\"✓ Fused detections: {len(fused_detections)}\")\n",
    "        \n",
    "        # Print detection details\n",
    "        print(\"\\n=== Detection Details ===\")\n",
    "        print(\"Camera detections:\")\n",
    "        for i, det in enumerate(camera_detections):\n",
    "            print(f\"  {i+1}. {det['class']} (conf: {det['confidence']:.3f})\")\n",
    "        \n",
    "        print(\"LiDAR detections:\")\n",
    "        for i, det in enumerate(lidar_detections):\n",
    "            print(f\"  {i+1}. {det['class']} (conf: {det['confidence']:.3f})\")\n",
    "            \n",
    "        print(\"Fused detections:\")\n",
    "        for i, det in enumerate(fused_detections):\n",
    "            print(f\"  {i+1}. {det['class']} (conf: {det['confidence']:.3f}, type: {det['fusion_type']})\")\n",
    "        \n",
    "        # Visualize results\n",
    "        if image is not None:\n",
    "            visualize_detections(\n",
    "                image, camera_detections, lidar_detections, fused_detections, calib,\n",
    "                save_path=f\"fusion_result_sample_{sample_idx}_v2.png\"\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'sample_idx': sample_idx,\n",
    "            'camera_detections': camera_detections,\n",
    "            'lidar_detections': lidar_detections,\n",
    "            'fused_detections': fused_detections,\n",
    "            'image': image,\n",
    "            'points': points,\n",
    "            'calibration': calib\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing sample {sample_idx}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test with the fixed version\n",
    "print(\"=== Testing Fixed LiDAR Processing ===\")\n",
    "test_result_v2 = process_single_sample_v2(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fcf0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenPCDet imported successfully\n",
      "✓ YOLOv5 imported successfully\n",
      "✓ YOLOv5 model loaded and moved to cuda\n",
      "✗ Error initializing PointPillars: [Errno 2] No such file or directory: '/path/to/pointpillar.yaml'\n",
      "✓ Late Fusion System initialized\n",
      "=== Processing Sample ===\n",
      "Image: 000000.png\n",
      "Point cloud: 000000.bin\n",
      "Calibration: 000000.txt\n",
      "✓ Calibration loaded\n",
      "Processing camera detections...\n",
      "✓ Camera detections: 1\n",
      "Processing LiDAR detections...\n",
      "✓ LiDAR detections: 0\n",
      "Performing late fusion...\n",
      "✓ Fused detections: 1\n",
      "\n",
      "=== Detection Summary ===\n",
      "Camera detections:\n",
      "  1. car (confidence: 0.565)\n",
      "LiDAR detections:\n",
      "Fused detections:\n",
      "  1. car (confidence: 0.565, type: camera_only, IoU: 0.000)\n",
      "✓ Visualization saved to: fusion_result.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nUSAGE INSTRUCTIONS:\\n\\n1. Basic Usage:\\n   \\n   config_paths = {\\n       \\'lidar_config_path\\': \\'/path/to/your/pointpillar.yaml\\',\\n       \\'lidar_checkpoint_path\\': \\'/path/to/your/pointpillar.pth\\'\\n   }\\n   \\n   result = quick_start(\"data\", config_paths)\\n\\n2. Advanced Usage:\\n   \\n   config = create_config_for_dataset(\\'kitti\\')\\n   config[\\'lidar_config_path\\'] = \\'/your/path/pointpillar.yaml\\'\\n   config[\\'lidar_checkpoint_path\\'] = \\'/your/path/pointpillar.pth\\'\\n   \\n   fusion_system = LateFusionSystem(config)\\n   results = fusion_system.process_dataset(\"data\", max_samples=10)\\n   \\n   # Analyze results\\n   stats = create_detection_statistics(results)\\n   save_results_to_file(results, \"results.json\")\\n\\n3. Required Files:\\n   - YOLOv5 model (downloaded automatically)\\n   - PointPillars config file (.yaml)\\n   - PointPillars checkpoint file (.pth)\\n   - Data in KITTI format:\\n     ├── data/\\n     │   ├── img/           # Camera images (.png)\\n     │   ├── velodyne/      # LiDAR point clouds (.bin)\\n     │   └── calib/         # Calibration files (.txt)\\n\\n4. Dependencies:\\n   pip install torch torchvision yolov5 opencv-python matplotlib numpy\\n   \\n   # OpenPCDet installation (follow official guide)\\n   git clone https://github.com/open-mmlab/OpenPCDet.git\\n   cd OpenPCDet\\n   pip install -v -e .\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete Late Fusion System for Camera and LiDAR Object Detection\n",
    "Combines YOLOv5 (camera) and PointPillars (LiDAR) detections using late fusion approach\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OpenPCDet imports\n",
    "try:\n",
    "    from pcdet.config import cfg, cfg_from_yaml_file\n",
    "    from pcdet.datasets import DatasetTemplate\n",
    "    from pcdet.models import build_network, load_data_to_gpu\n",
    "    from pcdet.utils import common_utils\n",
    "    print(\"✓ OpenPCDet imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ OpenPCDet import error: {e}\")\n",
    "    print(\"Please ensure OpenPCDet is properly installed\")\n",
    "\n",
    "# YOLOv5 imports\n",
    "try:\n",
    "    import yolov5\n",
    "    print(\"✓ YOLOv5 imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ YOLOv5 import error: {e}\")\n",
    "    print(\"Please install YOLOv5: pip install yolov5\")\n",
    "\n",
    "class KITTICalibration:\n",
    "    \"\"\"KITTI calibration file parser for coordinate transformations\"\"\"\n",
    "    \n",
    "    def __init__(self, calib_file):\n",
    "        \"\"\"Load calibration data from KITTI calibration file\"\"\"\n",
    "        calibs = self.read_calib_file(calib_file)\n",
    "        \n",
    "        # Camera intrinsics and extrinsics\n",
    "        self.P2 = calibs['P2'].reshape(3, 4)  # Camera 2 projection matrix\n",
    "        self.R0_rect = calibs['R0_rect'].reshape(3, 3)  # Rectification matrix\n",
    "        self.Tr_velo_to_cam = calibs['Tr_velo_to_cam'].reshape(3, 4)  # Velodyne to camera\n",
    "        \n",
    "        # Create full 4x4 transformation matrices\n",
    "        self.R0_rect_4x4 = np.eye(4)\n",
    "        self.R0_rect_4x4[:3, :3] = self.R0_rect\n",
    "        \n",
    "        self.Tr_velo_to_cam_4x4 = np.eye(4)\n",
    "        self.Tr_velo_to_cam_4x4[:3, :] = self.Tr_velo_to_cam\n",
    "        \n",
    "    def read_calib_file(self, filepath):\n",
    "        \"\"\"Read KITTI calibration file and parse parameters\"\"\"\n",
    "        data = {}\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.rstrip()\n",
    "                if len(line) == 0: \n",
    "                    continue\n",
    "                key, value = line.split(':', 1)\n",
    "                try:\n",
    "                    data[key] = np.array([float(x) for x in value.split()])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return data\n",
    "    \n",
    "    def project_velo_to_image(self, pts_3d_velo):\n",
    "        \"\"\"Project Velodyne 3D points to camera image coordinates\"\"\"\n",
    "        # Convert to homogeneous coordinates\n",
    "        pts_3d_velo_homo = np.hstack([pts_3d_velo, np.ones((pts_3d_velo.shape[0], 1))])\n",
    "        \n",
    "        # Transform: Velodyne -> Camera -> Rectified Camera -> Image\n",
    "        pts_3d_cam = pts_3d_velo_homo @ self.Tr_velo_to_cam_4x4.T\n",
    "        pts_3d_rect = pts_3d_cam @ self.R0_rect_4x4.T\n",
    "        pts_2d_image = pts_3d_rect @ self.P2.T\n",
    "        \n",
    "        # Normalize homogeneous coordinates\n",
    "        valid_mask = pts_2d_image[:, 2] > 0\n",
    "        pts_2d_image[valid_mask, 0] /= pts_2d_image[valid_mask, 2]\n",
    "        pts_2d_image[valid_mask, 1] /= pts_2d_image[valid_mask, 2]\n",
    "        \n",
    "        return pts_2d_image[:, :2], pts_3d_rect[:, 2]  # Return 2D points and depths\n",
    "\n",
    "class CameraDetector:\n",
    "    \"\"\"YOLOv5-based camera object detector\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='yolov5s', confidence_threshold=0.4, iou_threshold=0.45):\n",
    "        \"\"\"Initialize YOLOv5 model\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        \n",
    "        # Load YOLOv5 model\n",
    "        try:\n",
    "            self.model = yolov5.load(f'{model_name}.pt')\n",
    "            self.model.eval()\n",
    "            self.model.conf = confidence_threshold\n",
    "            self.model.iou = iou_threshold\n",
    "            \n",
    "            # Move to GPU if available\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            self.model.to(self.device)\n",
    "            print(f\"✓ YOLOv5 model loaded and moved to {self.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading YOLOv5: {e}\")\n",
    "            self.model = None\n",
    "    \n",
    "    def detect(self, image_path):\n",
    "        \"\"\"Detect objects in camera image\"\"\"\n",
    "        if self.model is None:\n",
    "            return [], None\n",
    "        \n",
    "        try:\n",
    "            # Load and process image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Error: Could not load image {image_path}\")\n",
    "                return [], None\n",
    "                \n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Run inference\n",
    "            results = self.model(image_rgb)\n",
    "            \n",
    "            # Extract detections\n",
    "            detections = []\n",
    "            if len(results.xyxy[0]) > 0:\n",
    "                for detection in results.xyxy[0].cpu().numpy():\n",
    "                    x1, y1, x2, y2, conf, cls = detection\n",
    "                    class_name = results.names[int(cls)]\n",
    "                    \n",
    "                    detections.append({\n",
    "                        'bbox': [float(x1), float(y1), float(x2), float(y2)],\n",
    "                        'confidence': float(conf),\n",
    "                        'class': class_name,\n",
    "                        'class_id': int(cls)\n",
    "                    })\n",
    "            \n",
    "            return detections, image_rgb\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in camera detection: {e}\")\n",
    "            return [], None\n",
    "\n",
    "class LiDARDetector:\n",
    "    \"\"\"PointPillars-based LiDAR object detector\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path, checkpoint_path, confidence_threshold=0.3):\n",
    "        \"\"\"Initialize PointPillars model\"\"\"\n",
    "        self.config_path = config_path\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        try:\n",
    "            # Load config\n",
    "            cfg_from_yaml_file(config_path, cfg)\n",
    "            \n",
    "            # Build model\n",
    "            self.dataset = DatasetTemplate(\n",
    "                dataset_cfg=cfg.DATA_CONFIG, \n",
    "                class_names=cfg.CLASS_NAMES, \n",
    "                training=False, \n",
    "                root_path=None, \n",
    "                logger=common_utils.create_logger()\n",
    "            )\n",
    "            \n",
    "            self.model = build_network(\n",
    "                model_cfg=cfg.MODEL, \n",
    "                num_class=len(cfg.CLASS_NAMES), \n",
    "                dataset=self.dataset\n",
    "            )\n",
    "            \n",
    "            # Load pretrained weights\n",
    "            self.model.load_params_from_file(checkpoint_path, logger=common_utils.create_logger())\n",
    "            self.model.to(self.device)\n",
    "            self.model.eval()\n",
    "            \n",
    "            print(f\"✓ PointPillars model loaded on {self.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error initializing PointPillars: {e}\")\n",
    "            self.model = None\n",
    "            self.dataset = None\n",
    "    \n",
    "    def load_pointcloud(self, bin_file):\n",
    "        \"\"\"Load KITTI point cloud from binary file\"\"\"\n",
    "        points = np.fromfile(bin_file, dtype=np.float32).reshape(-1, 4)\n",
    "        return points\n",
    "    \n",
    "    def prepare_input(self, points):\n",
    "        \"\"\"Prepare point cloud for PointPillars inference\"\"\"\n",
    "        try:\n",
    "            # Ensure points are float32 and have intensity channel\n",
    "            points = points.astype(np.float32)\n",
    "            if points.shape[1] == 3:\n",
    "                # Add dummy intensity if missing\n",
    "                intensity = np.ones((points.shape[0], 1), dtype=np.float32)\n",
    "                points = np.hstack([points, intensity])\n",
    "            \n",
    "            # Create input dict\n",
    "            input_dict = {\n",
    "                'points': points,\n",
    "                'frame_id': 0,\n",
    "            }\n",
    "            \n",
    "            # Prepare data using dataset\n",
    "            data_dict = self.dataset.prepare_data(input_dict)\n",
    "            \n",
    "            # Convert numpy arrays to tensors\n",
    "            for key, value in data_dict.items():\n",
    "                if isinstance(value, np.ndarray):\n",
    "                    data_dict[key] = torch.from_numpy(value)\n",
    "            \n",
    "            # Collate into batch\n",
    "            batch_dict = self.dataset.collate_batch([data_dict])\n",
    "            \n",
    "            # Move to device\n",
    "            for k, v in batch_dict.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    batch_dict[k] = v.to(self.device)\n",
    "                elif isinstance(v, np.ndarray):\n",
    "                    batch_dict[k] = torch.from_numpy(v).to(self.device)\n",
    "            \n",
    "            return batch_dict\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing LiDAR input: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def detect(self, point_file):\n",
    "        \"\"\"Detect objects in LiDAR point cloud\"\"\"\n",
    "        if self.model is None or self.dataset is None:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Load point cloud\n",
    "            points = self.load_pointcloud(point_file)\n",
    "            \n",
    "            # Prepare input\n",
    "            batch_dict = self.prepare_input(points)\n",
    "            if batch_dict is None:\n",
    "                return []\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                pred_dicts, _ = self.model(batch_dict)\n",
    "            \n",
    "            # Extract detections\n",
    "            detections = []\n",
    "            if len(pred_dicts) > 0:\n",
    "                pred_dict = pred_dicts[0]\n",
    "                \n",
    "                if 'pred_boxes' in pred_dict and len(pred_dict['pred_boxes']) > 0:\n",
    "                    boxes = pred_dict['pred_boxes'].cpu().numpy()\n",
    "                    scores = pred_dict['pred_scores'].cpu().numpy()\n",
    "                    labels = pred_dict['pred_labels'].cpu().numpy()\n",
    "                    \n",
    "                    # Filter by confidence threshold\n",
    "                    for box, score, label in zip(boxes, scores, labels):\n",
    "                        if score > self.confidence_threshold:\n",
    "                            # Ensure valid label\n",
    "                            if 0 < label <= len(cfg.CLASS_NAMES):\n",
    "                                class_name = cfg.CLASS_NAMES[label-1]\n",
    "                            else:\n",
    "                                class_name = f\"class_{label}\"\n",
    "                            \n",
    "                            detections.append({\n",
    "                                'bbox_3d': box,\n",
    "                                'confidence': float(score),\n",
    "                                'class': class_name,\n",
    "                                'class_id': int(label-1)\n",
    "                            })\n",
    "            \n",
    "            return detections\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in LiDAR detection: {e}\")\n",
    "            return []\n",
    "\n",
    "class LateFusionProcessor:\n",
    "    \"\"\"Late fusion processor for camera and LiDAR detections\"\"\"\n",
    "    \n",
    "    def __init__(self, iou_threshold=0.1, camera_weight=0.6, lidar_weight=0.6):\n",
    "        \"\"\"Initialize fusion processor with parameters\"\"\"\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.camera_weight = camera_weight\n",
    "        self.lidar_weight = lidar_weight\n",
    "        \n",
    "    def compute_2d_iou(self, box1, box2):\n",
    "        \"\"\"Compute IoU between two 2D bounding boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = (x2 - x1) * (y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def get_3d_box_corners(self, x, y, z, w, l, h, rot):\n",
    "        \"\"\"Get 3D bounding box corners from center and dimensions\"\"\"\n",
    "        # Create box corners in object coordinate system\n",
    "        corners = np.array([\n",
    "            [-l/2, -w/2, -h/2], [l/2, -w/2, -h/2],\n",
    "            [l/2, w/2, -h/2], [-l/2, w/2, -h/2],\n",
    "            [-l/2, -w/2, h/2], [l/2, -w/2, h/2],\n",
    "            [l/2, w/2, h/2], [-l/2, w/2, h/2]\n",
    "        ])\n",
    "        \n",
    "        # Rotation matrix around Z-axis\n",
    "        cos_rot = np.cos(rot)\n",
    "        sin_rot = np.sin(rot)\n",
    "        rot_matrix = np.array([\n",
    "            [cos_rot, -sin_rot, 0],\n",
    "            [sin_rot, cos_rot, 0],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        # Rotate and translate\n",
    "        corners = corners @ rot_matrix.T\n",
    "        corners[:, 0] += x\n",
    "        corners[:, 1] += y\n",
    "        corners[:, 2] += z\n",
    "        \n",
    "        return corners\n",
    "    \n",
    "    def project_3d_to_2d_bbox(self, bbox_3d, calib):\n",
    "        \"\"\"Project 3D bounding box to 2D image coordinates\"\"\"\n",
    "        try:\n",
    "            # Extract 3D box parameters\n",
    "            x, y, z, w, l, h, rot = bbox_3d[:7]\n",
    "            \n",
    "            # Get 3D box corners\n",
    "            corners_3d = self.get_3d_box_corners(x, y, z, w, l, h, rot)\n",
    "            \n",
    "            # Project to image\n",
    "            corners_2d, depths = calib.project_velo_to_image(corners_3d)\n",
    "            \n",
    "            # Filter points behind camera\n",
    "            valid_mask = depths > 0\n",
    "            if not np.any(valid_mask):\n",
    "                return None\n",
    "            \n",
    "            valid_corners = corners_2d[valid_mask]\n",
    "            \n",
    "            # Get 2D bounding box\n",
    "            if len(valid_corners) > 0:\n",
    "                x_min, y_min = np.min(valid_corners, axis=0)\n",
    "                x_max, y_max = np.max(valid_corners, axis=0)\n",
    "                return [x_min, y_min, x_max, y_max]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error projecting 3D box: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fuse_detections(self, camera_detections, lidar_detections, calibration):\n",
    "        \"\"\"Perform late fusion of camera and LiDAR detections\"\"\"\n",
    "        fused_detections = []\n",
    "        used_lidar = set()\n",
    "        \n",
    "        # Process camera detections and find matching LiDAR detections\n",
    "        for cam_det in camera_detections:\n",
    "            best_match = None\n",
    "            best_iou = 0\n",
    "            best_lidar_idx = -1\n",
    "            \n",
    "            # Find best matching LiDAR detection\n",
    "            for i, lidar_det in enumerate(lidar_detections):\n",
    "                if i in used_lidar:\n",
    "                    continue\n",
    "                \n",
    "                # Project 3D box to 2D\n",
    "                bbox_2d = self.project_3d_to_2d_bbox(lidar_det['bbox_3d'], calibration)\n",
    "                \n",
    "                if bbox_2d is not None:\n",
    "                    iou = self.compute_2d_iou(cam_det['bbox'], bbox_2d)\n",
    "                    \n",
    "                    if iou > best_iou and iou > self.iou_threshold:\n",
    "                        best_iou = iou\n",
    "                        best_match = lidar_det\n",
    "                        best_lidar_idx = i\n",
    "            \n",
    "            # Create fused detection\n",
    "            if best_match is not None:\n",
    "                # Fuse confidences\n",
    "                fused_confidence = (\n",
    "                    self.camera_weight * cam_det['confidence'] +\n",
    "                    self.lidar_weight * best_match['confidence']\n",
    "                )\n",
    "                \n",
    "                fused_detection = {\n",
    "                    'bbox_2d': cam_det['bbox'],\n",
    "                    'bbox_3d': best_match['bbox_3d'],\n",
    "                    'confidence': fused_confidence,\n",
    "                    'camera_confidence': cam_det['confidence'],\n",
    "                    'lidar_confidence': best_match['confidence'],\n",
    "                    'class': cam_det['class'],\n",
    "                    'fusion_type': 'camera_lidar',\n",
    "                    'iou': best_iou\n",
    "                }\n",
    "                \n",
    "                fused_detections.append(fused_detection)\n",
    "                used_lidar.add(best_lidar_idx)\n",
    "            else:\n",
    "                # Camera-only detection\n",
    "                fused_detection = {\n",
    "                    'bbox_2d': cam_det['bbox'],\n",
    "                    'bbox_3d': None,\n",
    "                    'confidence': cam_det['confidence'],\n",
    "                    'camera_confidence': cam_det['confidence'],\n",
    "                    'lidar_confidence': 0.0,\n",
    "                    'class': cam_det['class'],\n",
    "                    'fusion_type': 'camera_only',\n",
    "                    'iou': 0.0\n",
    "                }\n",
    "                fused_detections.append(fused_detection)\n",
    "        \n",
    "        # Add unmatched LiDAR detections\n",
    "        for i, lidar_det in enumerate(lidar_detections):\n",
    "            if i not in used_lidar:\n",
    "                fused_detection = {\n",
    "                    'bbox_2d': None,\n",
    "                    'bbox_3d': lidar_det['bbox_3d'],\n",
    "                    'confidence': lidar_det['confidence'],\n",
    "                    'camera_confidence': 0.0,\n",
    "                    'lidar_confidence': lidar_det['confidence'],\n",
    "                    'class': lidar_det['class'],\n",
    "                    'fusion_type': 'lidar_only',\n",
    "                    'iou': 0.0\n",
    "                }\n",
    "                fused_detections.append(fused_detection)\n",
    "        \n",
    "        return fused_detections\n",
    "\n",
    "class VisualizationManager:\n",
    "    \"\"\"Handle visualization of detection results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.colors = {\n",
    "            'camera_lidar': (255, 0, 255),  # Magenta\n",
    "            'camera_only': (255, 0, 0),     # Red\n",
    "            'lidar_only': (0, 255, 0)       # Green\n",
    "        }\n",
    "    \n",
    "    def draw_2d_bbox(self, image, bbox, color, label, thickness=2):\n",
    "        \"\"\"Draw 2D bounding box on image\"\"\"\n",
    "        x1, y1, x2, y2 = [int(coord) for coord in bbox]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness)\n",
    "        \n",
    "        # Add label\n",
    "        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "        cv2.rectangle(image, (x1, y1 - label_size[1] - 10), \n",
    "                     (x1 + label_size[0], y1), color, -1)\n",
    "        cv2.putText(image, label, (x1, y1 - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    def visualize_detections(self, image, camera_dets, lidar_dets, fused_dets, \n",
    "                           calibration, fusion_processor, save_path=None):\n",
    "        \"\"\"Create comprehensive visualization of all detections\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, 0].imshow(image)\n",
    "        axes[0, 0].set_title('Original Image', fontsize=14)\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # Camera detections\n",
    "        img_cam = image.copy()\n",
    "        for det in camera_dets:\n",
    "            label = f\"{det['class']}: {det['confidence']:.2f}\"\n",
    "            self.draw_2d_bbox(img_cam, det['bbox'], self.colors['camera_only'], label)\n",
    "        \n",
    "        axes[0, 1].imshow(img_cam)\n",
    "        axes[0, 1].set_title(f'Camera Detections ({len(camera_dets)})', fontsize=14)\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # LiDAR detections projected to image\n",
    "        img_lidar = image.copy()\n",
    "        for det in lidar_dets:\n",
    "            bbox_2d = fusion_processor.project_3d_to_2d_bbox(det['bbox_3d'], calibration)\n",
    "            if bbox_2d is not None:\n",
    "                label = f\"{det['class']}: {det['confidence']:.2f}\"\n",
    "                self.draw_2d_bbox(img_lidar, bbox_2d, self.colors['lidar_only'], label)\n",
    "        \n",
    "        axes[1, 0].imshow(img_lidar)\n",
    "        axes[1, 0].set_title(f'LiDAR Detections Projected ({len(lidar_dets)})', fontsize=14)\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # Fused detections\n",
    "        img_fused = image.copy()\n",
    "        for det in fused_dets:\n",
    "            color = self.colors[det['fusion_type']]\n",
    "            \n",
    "            if det['bbox_2d'] is not None:\n",
    "                label = f\"{det['class']}: {det['confidence']:.2f} ({det['fusion_type']})\"\n",
    "                self.draw_2d_bbox(img_fused, det['bbox_2d'], color, label)\n",
    "            elif det['bbox_3d'] is not None:\n",
    "                # Project LiDAR-only detection\n",
    "                bbox_2d = fusion_processor.project_3d_to_2d_bbox(det['bbox_3d'], calibration)\n",
    "                if bbox_2d is not None:\n",
    "                    label = f\"{det['class']}: {det['confidence']:.2f} ({det['fusion_type']})\"\n",
    "                    self.draw_2d_bbox(img_fused, bbox_2d, color, label)\n",
    "        \n",
    "        axes[1, 1].imshow(img_fused)\n",
    "        axes[1, 1].set_title(f'Fused Detections ({len(fused_dets)})', fontsize=14)\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='red', alpha=0.7, label='Camera Only'),\n",
    "            Patch(facecolor='green', alpha=0.7, label='LiDAR Only'),\n",
    "            Patch(facecolor='magenta', alpha=0.7, label='Camera + LiDAR')\n",
    "        ]\n",
    "        axes[1, 1].legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "            print(f\"✓ Visualization saved to: {save_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "class LateFusionSystem:\n",
    "    \"\"\"Main system class that orchestrates the complete late fusion pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initialize the complete fusion system\"\"\"\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize components\n",
    "        self.camera_detector = CameraDetector(\n",
    "            model_name=config.get('camera_model', 'yolov5s'),\n",
    "            confidence_threshold=config.get('camera_confidence', 0.4)\n",
    "        )\n",
    "        \n",
    "        self.lidar_detector = LiDARDetector(\n",
    "            config_path=config['lidar_config_path'],\n",
    "            checkpoint_path=config['lidar_checkpoint_path'],\n",
    "            confidence_threshold=config.get('lidar_confidence', 0.3)\n",
    "        )\n",
    "        \n",
    "        self.fusion_processor = LateFusionProcessor(\n",
    "            iou_threshold=config.get('iou_threshold', 0.1),\n",
    "            camera_weight=config.get('camera_weight', 0.6),\n",
    "            lidar_weight=config.get('lidar_weight', 0.6)\n",
    "        )\n",
    "        \n",
    "        self.visualizer = VisualizationManager()\n",
    "        \n",
    "        print(\"✓ Late Fusion System initialized\")\n",
    "    \n",
    "    def process_sample(self, image_path, pointcloud_path, calibration_path, \n",
    "                      visualize=True, save_path=None):\n",
    "        \"\"\"Process a single sample with camera and LiDAR data\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"=== Processing Sample ===\")\n",
    "            print(f\"Image: {os.path.basename(image_path)}\")\n",
    "            print(f\"Point cloud: {os.path.basename(pointcloud_path)}\")\n",
    "            print(f\"Calibration: {os.path.basename(calibration_path)}\")\n",
    "            \n",
    "            # Load calibration\n",
    "            calibration = KITTICalibration(calibration_path)\n",
    "            print(\"✓ Calibration loaded\")\n",
    "            \n",
    "            # Camera detection\n",
    "            print(\"Processing camera detections...\")\n",
    "            camera_detections, image = self.camera_detector.detect(image_path)\n",
    "            print(f\"✓ Camera detections: {len(camera_detections)}\")\n",
    "            \n",
    "            # LiDAR detection\n",
    "            print(\"Processing LiDAR detections...\")\n",
    "            lidar_detections = self.lidar_detector.detect(pointcloud_path)\n",
    "            print(f\"✓ LiDAR detections: {len(lidar_detections)}\")\n",
    "            \n",
    "            # Fusion\n",
    "            print(\"Performing late fusion...\")\n",
    "            fused_detections = self.fusion_processor.fuse_detections(\n",
    "                camera_detections, lidar_detections, calibration\n",
    "            )\n",
    "            print(f\"✓ Fused detections: {len(fused_detections)}\")\n",
    "            \n",
    "            # Print detailed results\n",
    "            self.print_detection_summary(camera_detections, lidar_detections, fused_detections)\n",
    "            \n",
    "            # Visualization\n",
    "            if visualize and image is not None:\n",
    "                self.visualizer.visualize_detections(\n",
    "                    image, camera_detections, lidar_detections, fused_detections,\n",
    "                    calibration, self.fusion_processor, save_path\n",
    "                )\n",
    "            \n",
    "            return {\n",
    "                'camera_detections': camera_detections,\n",
    "                'lidar_detections': lidar_detections,\n",
    "                'fused_detections': fused_detections,\n",
    "                'image': image,\n",
    "                'calibration': calibration\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error processing sample: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    \n",
    "    def print_detection_summary(self, camera_dets, lidar_dets, fused_dets):\n",
    "        \"\"\"Print detailed summary of detections\"\"\"\n",
    "        print(\"\\n=== Detection Summary ===\")\n",
    "        \n",
    "        print(\"Camera detections:\")\n",
    "        for i, det in enumerate(camera_dets):\n",
    "            print(f\"  {i+1}. {det['class']} (confidence: {det['confidence']:.3f})\")\n",
    "        \n",
    "        print(\"LiDAR detections:\")\n",
    "        for i, det in enumerate(lidar_dets):\n",
    "            print(f\"  {i+1}. {det['class']} (confidence: {det['confidence']:.3f})\")\n",
    "        \n",
    "        print(\"Fused detections:\")\n",
    "        for i, det in enumerate(fused_dets):\n",
    "            print(f\"  {i+1}. {det['class']} (confidence: {det['confidence']:.3f}, \"\n",
    "                  f\"type: {det['fusion_type']}, IoU: {det['iou']:.3f})\")\n",
    "    \n",
    "    def process_dataset(self, data_directory, max_samples=None, save_results=True):\n",
    "        \"\"\"Process multiple samples from a dataset directory\"\"\"\n",
    "        \n",
    "        # Find data files\n",
    "        image_files = sorted(glob.glob(os.path.join(data_directory, \"img\", \"*.png\")))\n",
    "        point_files = sorted(glob.glob(os.path.join(data_directory, \"velodyne\", \"*.bin\")))\n",
    "        calib_files = sorted(glob.glob(os.path.join(data_directory, \"calib\", \"*.txt\")))\n",
    "        \n",
    "        # Validate data consistency\n",
    "        num_samples = min(len(image_files), len(point_files), len(calib_files))\n",
    "        if max_samples:\n",
    "            num_samples = min(num_samples, max_samples)\n",
    "        \n",
    "        print(f\"\\n=== Processing Dataset ===\")\n",
    "        print(f\"Found {len(image_files)} images, {len(point_files)} point clouds, \"\n",
    "              f\"{len(calib_files)} calibration files\")\n",
    "        print(f\"Processing {num_samples} samples...\")\n",
    "        \n",
    "        results = []\n",
    "        for i in range(num_samples):\n",
    "            print(f\"\\n--- Sample {i+1}/{num_samples} ---\")\n",
    "            \n",
    "            result = self.process_sample(\n",
    "                image_files[i], point_files[i], calib_files[i],\n",
    "                visualize=False,  # Don't visualize during batch processing\n",
    "                save_path=f\"fusion_result_{i:06d}.png\" if save_results else None\n",
    "            )\n",
    "            \n",
    "            if result:\n",
    "                results.append(result)\n",
    "        \n",
    "        print(f\"\\n✓ Processed {len(results)} samples successfully\")\n",
    "        return results\n",
    "\n",
    "# Example usage and configuration\n",
    "def main():\n",
    "    \"\"\"Main function demonstrating system usage\"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        'camera_model': 'yolov5s',\n",
    "        'camera_confidence': 0.4,\n",
    "        'lidar_config_path': '/path/to/pointpillar.yaml',  # Update this path\n",
    "        'lidar_checkpoint_path': '/path/to/pointpillar.pth',  # Update this path\n",
    "        'lidar_confidence': 0.3,\n",
    "        'iou_threshold': 0.1,\n",
    "        'camera_weight': 0.6,\n",
    "        'lidar_weight': 0.6\n",
    "    }\n",
    "    \n",
    "    # Initialize system\n",
    "    fusion_system = LateFusionSystem(config)\n",
    "    \n",
    "    # Process single sample\n",
    "    image_path = \"data/img/000000.png\"\n",
    "    pointcloud_path = \"data/velodyne/000000.bin\"\n",
    "    calibration_path = \"data/calib/000000.txt\"\n",
    "    \n",
    "    result = fusion_system.process_sample(\n",
    "        image_path, pointcloud_path, calibration_path,\n",
    "        visualize=True, save_path=\"fusion_result.png\"\n",
    "    )\n",
    "    \n",
    "    # Process entire dataset (optional)\n",
    "    # results = fusion_system.process_dataset(\"data\", max_samples=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "# Additional utility functions for advanced usage\n",
    "\n",
    "def create_detection_statistics(results):\n",
    "    \"\"\"Analyze detection statistics across multiple samples\"\"\"\n",
    "    \n",
    "    fusion_types = ['camera_only', 'lidar_only', 'camera_lidar']\n",
    "    type_counts = {ft: 0 for ft in fusion_types}\n",
    "    confidences = {ft: [] for ft in fusion_types}\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    for result in results:\n",
    "        for det in result['fused_detections']:\n",
    "            fusion_type = det['fusion_type']\n",
    "            type_counts[fusion_type] += 1\n",
    "            confidences[fusion_type].append(det['confidence'])\n",
    "            class_counts[det['class']] += 1\n",
    "    \n",
    "    # Plot statistics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Detection type distribution\n",
    "    axes[0, 0].bar(type_counts.keys(), type_counts.values(), \n",
    "                   color=['red', 'green', 'magenta'], alpha=0.7)\n",
    "    axes[0, 0].set_title('Detection Type Distribution')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Confidence distribution\n",
    "    for ft, confs in confidences.items():\n",
    "        if confs:\n",
    "            axes[0, 1].hist(confs, alpha=0.7, label=ft, bins=20)\n",
    "    axes[0, 1].set_title('Confidence Distribution by Fusion Type')\n",
    "    axes[0, 1].set_xlabel('Confidence')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Class distribution\n",
    "    classes = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    axes[1, 0].bar(classes, counts, alpha=0.7)\n",
    "    axes[1, 0].set_title('Object Class Distribution')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Fusion effectiveness (IoU distribution for fused detections)\n",
    "    ious = []\n",
    "    for result in results:\n",
    "        for det in result['fused_detections']:\n",
    "            if det['fusion_type'] == 'camera_lidar' and det['iou'] > 0:\n",
    "                ious.append(det['iou'])\n",
    "    \n",
    "    if ious:\n",
    "        axes[1, 1].hist(ious, bins=20, alpha=0.7, color='purple')\n",
    "        axes[1, 1].set_title('IoU Distribution for Fused Detections')\n",
    "        axes[1, 1].set_xlabel('IoU')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No fused detections', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('IoU Distribution for Fused Detections')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'type_counts': type_counts,\n",
    "        'confidences': confidences,\n",
    "        'class_counts': dict(class_counts),\n",
    "        'ious': ious\n",
    "    }\n",
    "\n",
    "def save_results_to_file(results, output_path):\n",
    "    \"\"\"Save detection results to JSON file\"\"\"\n",
    "    \n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    serializable_results = []\n",
    "    \n",
    "    for result in results:\n",
    "        serializable_result = {\n",
    "            'camera_detections': [],\n",
    "            'lidar_detections': [],\n",
    "            'fused_detections': []\n",
    "        }\n",
    "        \n",
    "        # Camera detections\n",
    "        for det in result['camera_detections']:\n",
    "            serializable_result['camera_detections'].append({\n",
    "                'bbox': det['bbox'],\n",
    "                'confidence': float(det['confidence']),\n",
    "                'class': det['class'],\n",
    "                'class_id': int(det['class_id'])\n",
    "            })\n",
    "        \n",
    "        # LiDAR detections\n",
    "        for det in result['lidar_detections']:\n",
    "            bbox_3d = det['bbox_3d'].tolist() if isinstance(det['bbox_3d'], np.ndarray) else det['bbox_3d']\n",
    "            serializable_result['lidar_detections'].append({\n",
    "                'bbox_3d': bbox_3d,\n",
    "                'confidence': float(det['confidence']),\n",
    "                'class': det['class'],\n",
    "                'class_id': int(det['class_id'])\n",
    "            })\n",
    "        \n",
    "        # Fused detections\n",
    "        for det in result['fused_detections']:\n",
    "            fused_det = {\n",
    "                'bbox_2d': det['bbox_2d'],\n",
    "                'confidence': float(det['confidence']),\n",
    "                'camera_confidence': float(det['camera_confidence']),\n",
    "                'lidar_confidence': float(det['lidar_confidence']),\n",
    "                'class': det['class'],\n",
    "                'fusion_type': det['fusion_type'],\n",
    "                'iou': float(det['iou'])\n",
    "            }\n",
    "            \n",
    "            if det['bbox_3d'] is not None:\n",
    "                bbox_3d = det['bbox_3d'].tolist() if isinstance(det['bbox_3d'], np.ndarray) else det['bbox_3d']\n",
    "                fused_det['bbox_3d'] = bbox_3d\n",
    "            else:\n",
    "                fused_det['bbox_3d'] = None\n",
    "                \n",
    "            serializable_result['fused_detections'].append(fused_det)\n",
    "        \n",
    "        serializable_results.append(serializable_result)\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Results saved to {output_path}\")\n",
    "\n",
    "def load_results_from_file(input_path):\n",
    "    \"\"\"Load detection results from JSON file\"\"\"\n",
    "    \n",
    "    with open(input_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Results loaded from {input_path}\")\n",
    "    return data\n",
    "\n",
    "class BenchmarkEvaluator:\n",
    "    \"\"\"Evaluate fusion system performance against ground truth\"\"\"\n",
    "    \n",
    "    def __init__(self, iou_threshold=0.5):\n",
    "        self.iou_threshold = iou_threshold\n",
    "    \n",
    "    def compute_metrics(self, predictions, ground_truth):\n",
    "        \"\"\"Compute precision, recall, and F1 score\"\"\"\n",
    "        \n",
    "        tp = 0  # True positives\n",
    "        fp = 0  # False positives\n",
    "        fn = 0  # False negatives\n",
    "        \n",
    "        # Match predictions with ground truth\n",
    "        matched_gt = set()\n",
    "        \n",
    "        for pred in predictions:\n",
    "            best_iou = 0\n",
    "            best_gt_idx = -1\n",
    "            \n",
    "            for i, gt in enumerate(ground_truth):\n",
    "                if i in matched_gt:\n",
    "                    continue\n",
    "                \n",
    "                # Compute IoU (simplified for 2D boxes)\n",
    "                if pred.get('bbox_2d') and gt.get('bbox'):\n",
    "                    iou = self.compute_2d_iou(pred['bbox_2d'], gt['bbox'])\n",
    "                    if iou > best_iou and pred['class'] == gt['class']:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = i\n",
    "            \n",
    "            if best_iou >= self.iou_threshold:\n",
    "                tp += 1\n",
    "                matched_gt.add(best_gt_idx)\n",
    "            else:\n",
    "                fp += 1\n",
    "        \n",
    "        fn = len(ground_truth) - len(matched_gt)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "    \n",
    "    def compute_2d_iou(self, box1, box2):\n",
    "        \"\"\"Compute IoU between two 2D boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = (x2 - x1) * (y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# Configuration templates for different scenarios\n",
    "\n",
    "KITTI_CONFIG = {\n",
    "    'camera_model': 'yolov5s',\n",
    "    'camera_confidence': 0.4,\n",
    "    'lidar_config_path': '/path/to/kitti_pointpillar.yaml',\n",
    "    'lidar_checkpoint_path': '/path/to/pointpillar_kitti.pth',\n",
    "    'lidar_confidence': 0.3,\n",
    "    'iou_threshold': 0.1,\n",
    "    'camera_weight': 0.6,\n",
    "    'lidar_weight': 0.6\n",
    "}\n",
    "\n",
    " \n",
    "\n",
    "def create_config_for_dataset(dataset_name):\n",
    "    \"\"\"Create configuration for specific dataset\"\"\"\n",
    "    \n",
    "    configs = {\n",
    "        'kitti': KITTI_CONFIG,\n",
    "        'nuscenes': NUSC_CONFIG\n",
    "    }\n",
    "    \n",
    "    if dataset_name.lower() in configs:\n",
    "        return configs[dataset_name.lower()].copy()\n",
    "    else:\n",
    "        print(f\"Warning: Unknown dataset {dataset_name}, using default KITTI config\")\n",
    "        return KITTI_CONFIG.copy()\n",
    "\n",
    "# Example of advanced usage with custom configuration\n",
    "def advanced_example():\n",
    "    \"\"\"Advanced example showing custom configuration and evaluation\"\"\"\n",
    "    \n",
    "    # Create custom configuration\n",
    "    custom_config = {\n",
    "        'camera_model': 'yolov5m',  # Use medium model for better accuracy\n",
    "        'camera_confidence': 0.5,\n",
    "        'lidar_config_path': '/home/manas/OpenPCDet/tools/cfgs/kitti_models/pointpillar.yaml',\n",
    "        'lidar_checkpoint_path': '/home/manas/Downloads/pointpillar_7728.pth',\n",
    "        'lidar_confidence': 0.4,\n",
    "        'iou_threshold': 0.15,  # Higher threshold for stricter matching\n",
    "        'camera_weight': 0.7,   # Give more weight to camera\n",
    "        'lidar_weight': 0.5\n",
    "    }\n",
    "    \n",
    "    # Initialize system\n",
    "    fusion_system = LateFusionSystem(custom_config)\n",
    "    \n",
    "    # Process dataset\n",
    "    data_dir = \"data\"\n",
    "    results = fusion_system.process_dataset(data_dir, max_samples=5)\n",
    "    \n",
    "    if results:\n",
    "        # Analyze statistics\n",
    "        stats = create_detection_statistics(results)\n",
    "        \n",
    "        # Save results\n",
    "        save_results_to_file(results, \"fusion_results.json\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n=== Final Summary ===\")\n",
    "        print(f\"Total samples processed: {len(results)}\")\n",
    "        print(f\"Detection type distribution: {stats['type_counts']}\")\n",
    "        print(f\"Average IoU for fused detections: {np.mean(stats['ious']):.3f}\" if stats['ious'] else \"No fused detections\")\n",
    "\n",
    "# Quick start function for easy setup\n",
    "def quick_start(data_directory, config_paths):\n",
    "    \"\"\"Quick start function for immediate usage\"\"\"\n",
    "    \n",
    "    # Update paths in config\n",
    "    config = KITTI_CONFIG.copy()\n",
    "    config.update(config_paths)\n",
    "    \n",
    "    # Initialize and run\n",
    "    fusion_system = LateFusionSystem(config)\n",
    "    \n",
    "    # Find first sample\n",
    "    image_files = sorted(glob.glob(os.path.join(data_directory, \"img\", \"*.png\")))\n",
    "    point_files = sorted(glob.glob(os.path.join(data_directory, \"velodyne\", \"*.bin\")))\n",
    "    calib_files = sorted(glob.glob(os.path.join(data_directory, \"calib\", \"*.txt\")))\n",
    "    \n",
    "    if image_files and point_files and calib_files:\n",
    "        result = fusion_system.process_sample(\n",
    "            image_files[0], point_files[0], calib_files[0],\n",
    "            visualize=True, save_path=\"quick_start_result.png\"\n",
    "        )\n",
    "        print(\"✓ Quick start completed successfully!\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"✗ Could not find data files in the specified directory\")\n",
    "        return None\n",
    "\n",
    "# Instructions for usage\n",
    "\"\"\"\n",
    "USAGE INSTRUCTIONS:\n",
    "\n",
    "1. Basic Usage:\n",
    "   \n",
    "   config_paths = {\n",
    "       'lidar_config_path': '/path/to/your/pointpillar.yaml',\n",
    "       'lidar_checkpoint_path': '/path/to/your/pointpillar.pth'\n",
    "   }\n",
    "   \n",
    "   result = quick_start(\"data\", config_paths)\n",
    "\n",
    "2. Advanced Usage:\n",
    "   \n",
    "   config = create_config_for_dataset('kitti')\n",
    "   config['lidar_config_path'] = '/your/path/pointpillar.yaml'\n",
    "   config['lidar_checkpoint_path'] = '/your/path/pointpillar.pth'\n",
    "   \n",
    "   fusion_system = LateFusionSystem(config)\n",
    "   results = fusion_system.process_dataset(\"data\", max_samples=10)\n",
    "   \n",
    "   # Analyze results\n",
    "   stats = create_detection_statistics(results)\n",
    "   save_results_to_file(results, \"results.json\")\n",
    "\n",
    "3. Required Files:\n",
    "   - YOLOv5 model (downloaded automatically)\n",
    "   - PointPillars config file (.yaml)\n",
    "   - PointPillars checkpoint file (.pth)\n",
    "   - Data in KITTI format:\n",
    "     ├── data/\n",
    "     │   ├── img/           # Camera images (.png)\n",
    "     │   ├── velodyne/      # LiDAR point clouds (.bin)\n",
    "     │   └── calib/         # Calibration files (.txt)\n",
    "\n",
    "4. Dependencies:\n",
    "   pip install torch torchvision yolov5 opencv-python matplotlib numpy\n",
    "   \n",
    "   # OpenPCDet installation (follow official guide)\n",
    "   git clone https://github.com/open-mmlab/OpenPCDet.git\n",
    "   cd OpenPCDet\n",
    "   pip install -v -e .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed51851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sensor_fusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
